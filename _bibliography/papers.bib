
@article{takabi_privacy_2019,
	title = {Privacy preserving neural network inference on encrypted data with {GPUs}},
	copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
	journal = {arXiv preprint arXiv:1911.11377},
	author = {Takabi, Daniel and Podschwadt, Robert and Druce, Jeff and Wu, Curt and Procopio, Kevin},
	year = {2019},
}

@inproceedings{podschwadt_effectiveness_2019,
	title = {On {Effectiveness} of {Adversarial} {Examples} and {Defenses} for {Malware} {Classification}},
	copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
	booktitle = {International {Conference} on {Security} and {Privacy} in {Communication} {Systems}},
	publisher = {Springer},
	author = {Podschwadt, Robert and Takabi, Hassan},
	year = {2019},
	pages = {380--393},
	
}

@inproceedings{podschwadt_classification_2020,
	title = {Classification of {Encrypted} {Word} {Embeddings} using {Recurrent} {Neural} {Networks}.},
	copyright = {Creative Commons Attribution 4.0 International License (CC-BY)},
	booktitle = {{PrivateNLP}@ {WSDM}},
	author = {Podschwadt, Robert and Takabi, Daniel},
	year = {2020},
	pages = {27--31},
	
}

@inproceedings{podschwadt_non-interactive_2021,
	title = {Non-interactive {Privacy} {Preserving} {Recurrent} {Neural} {Network} {Prediction} with {Homomorphic} {Encryption}},
	copyright = {All rights reserved},
	booktitle = {2021 {IEEE} 14th {International} {Conference} on {Cloud} {Computing} ({CLOUD})},
	publisher = {IEEE},
	author = {Podschwadt, Robert and Takabi, Daniel},
	year = {2021},
	pages = {65--70},
	
}

@article{podschwadt_survey_2022,
	title = {A {Survey} of {Deep} {Learning} {Architectures} for {Privacy}-{Preserving} {Machine} {Learning} {With} {Fully} {Homomorphic} {Encryption}},
	volume = {10},
	copyright = {All rights reserved},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2022.3219049},
	abstract = {Outsourced computation for neural networks allows users access to state-of-the-art models without investing in specialized hardware and know-how. The problem is that the users lose control over potentially privacy-sensitive data. With homomorphic encryption (HE), a third party can perform computation on encrypted data without revealing its content. In this paper, we reviewed scientific articles and publications in the particular area of Deep Learning Architectures for Privacy-Preserving Machine Learning (PPML) with Fully HE. We analyzed the changes to neural network models and architectures to make them compatible with HE and how these changes impact performance. Next, we find numerous challenges to HE-based privacy-preserving deep learning, such as computational overhead, usability, and limitations posed by the encryption schemes. Furthermore, we discuss potential solutions to the HE PPML challenges. Finally, we propose evaluation metrics that allow for a better and more meaningful comparison of PPML solutions.},
	journal = {IEEE Access},
	author = {Podschwadt, Robert and Takabi, Daniel and Hu, Peizhao and Rafiei, Mohammad H. and Cai, Zhipeng},
	year = {2022},
	note = {Conference Name: IEEE Access},
	keywords = {homomorphic encryption, Homomorphic encryption, Privacy, Public key, neural networks, Machine learning, Neural networks, Servers, Computational modeling, Data models, Artificial neural networks, Deep learning, machine learning, privacy, privacy preservation, Differential privacy},
	pages = {117477--117500},
}
